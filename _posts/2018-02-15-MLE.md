---
layout: post
title: MLE
tags: [Probability theory]
categories: [Probability theory]
excerpt_separator: <!--more-->

---

현실의 문제에서 확률분포를 완전히 파악하고 있는 경우는 드물다. 따라서 **샘플링을 통해 해당 확률밀도함수의 모수를 추정**한다. 이 포스트에서는 모수추정법 중 가장 널리 알려진 **MLE**에 대해 정리해본다. 
<!--more-->

* TOC
{:toc}

## 개요
### Likelihood와 Log-likelihood
확률분포는 확률밀도함수(pdf: probability density function) 또는 확률질량함수(pmf: probability mass function)를 통해 묘사된다. 어떤 확률변수 <span><script type="math/tex">X</script></span>가 모수 <span><script type="math/tex">\theta</script></span>의 확률밀도함수 <span><script type="math/tex">\mathbf{F}</script></span>를 따른다고 하자. 이 확률분포에 대한 모든 정보, 즉 **모수 <span><script type="math/tex">\theta</script></span>를 이미 알고 있다고 가정**하면, 이 경우 확률밀도함수 <span><script type="math/tex">\mathbf{F}</script></span>는 <span><script type="math/tex">x</script></span>의 함수가 된다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{F}(x; \theta) = \Pr[X=x] \rightarrow {\small\it\text{function of}} ~x
</script></div>

하지만 **확률분포의 모수 <span><script type="math/tex">\theta</script></span>를 안다는 건 현실적으로 불가능**하다. 대부분의 문제에서는, 확률변수 <span><script type="math/tex">X</script></span>로부터 도출된 샘플 <span><script type="math/tex">x</script></span> 이 주어지고, 이를 통해 모수 <span><script type="math/tex">\theta</script></span>를 추정해야 되는 상황에 직면한다. 확률밀도함수 <span><script type="math/tex">\mathbf{F}</script></span>를 (<span><script type="math/tex">x</script></span>의 함수가 아닌) <span><script type="math/tex">\theta</script></span>의 함수로 해석하는 것을 **Likelihood**[^likelihood] <span><script type="math/tex">\mathcal{L}</script></span> 라고 부른다. 

<div class="math"><script type="math/tex; mode=display">
\mathcal{L} (\theta ; x) = \mathbf{F}(x; \theta) \rightarrow {\small\it\text{function of}} ~\theta
</script></div>

[^likelihood]: 우도(尤度) 함수 또는 가능도(可能度) 함수라고도 한다. 

결국 확률밀도함수와 Likelihood는 같은 형태라고 할 수 있다. 샘플 <span><script type="math/tex">x</script></span>의 함수로 볼 것인가, 아니면 모수 <span><script type="math/tex">\theta</script></span>의 함수로 볼 것인가의 차이일 뿐이다. 



그런데 샘플 <span><script type="math/tex">x</script></span> 하나만 가지고 확률분포를 추정하는 일은 현실적으로 없을 것이다. 대부분의 상황에서는 확률변수 <span><script type="math/tex">X</script></span>로부터 추출된 여러 개의 샘플들, 이를테면 <span><script type="math/tex">n</script></span>개의 샘플 <span><script type="math/tex">\mathbf{x} = (x_1, \cdots x_n)</script></span>을 통해 모수를 추정한다. 만약 **<span><script type="math/tex">n</script></span>개의 샘플링이 [독립시행](https://en.wikipedia.org/wiki/Independence_(probability_theory)) 되었다고 가정**한다면, Likelihood <span><script type="math/tex">\mathcal{L}</script></span>은 다음과 같이 전개된다. 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}(\theta; \mathbf{x})
&= \Pr[x_1, \cdots, x_n ] \\
&= \prod_{i=1}^n \Pr [X=x_i] \\
&= \prod_{i=1}^n \mathbf{F}(x_i ; \theta)
\end{aligned}
</script></div>

이처럼 샘플이 많은 경우의 Likelihood는 모두 곱셈으로 연결되어 있어, 이후의 대수적인 수식전개가 쉽지 않을 것이다. Likelihood에 로그를 씌워서 수식구조를 덧셈으로 변환한 것을 **Log-likelihood** 라고 한다. 

<div class="math"><script type="math/tex; mode=display">
\ln \mathcal{L} (\theta; \mathbf{x}) = \ln \left( \prod_{i=1}^n \mathbf{F}(x_i ; \theta) \right) = \sum_{i=1}^n \ln \left[ \mathbf{F}(x_i; \theta) \right]
</script></div>


### MLE 프로세스
어떤 확률변수에서의 샘플값들을 토대로 그 확률변수의 모수를 구하는 것을 **모수추정**이라고 한다. **MLE** (Maximum Likelihood Estimation[^mle])는 모수추정 방법론 중 가장 일반적으로 사용되는 것이며, **주어진 샘플들이 추출될 Likelihood를 최대로 만드는 모수를 선택**한다. 즉 샘플들 <span><script type="math/tex">\mathbf{x} = (x_1, \cdots, x_n)</script></span> 에 대해, 

[^mle]: 최대우도 추정법 또는 최대가능도 추정법이라고도 한다. 

<div class="math"><script type="math/tex; mode=display">
\hat{\theta} = \underset{\theta}{\arg \max} \mathcal{L} (\theta; \mathbf{x}) 
</script></div>

가 된다. 여기서 <span><script type="math/tex">\hat\theta</script></span>는 <span><script type="math/tex">\theta</script></span>의 추정치라는 의미의 부호이다. 이 값은 다음의 방정식을 풀면 구해진다. 

<div class="math"><script type="math/tex; mode=display">
\frac{\partial}{\partial \theta} \mathcal{L} (\theta; \mathbf{x}) \Big|_{\hat\theta} = \frac{\partial}{\partial \theta} \left[ \prod_{i=1}^n \mathbf{F}(x_i ; \theta)  \right]_{\hat\theta} = 0
</script></div>

**만약 모든 샘플들이 독립적으로 선택**(즉 독립시행)되었다면, 위의 방정식은 좀 더 다루기 쉬운 형태로 바꿀 수 있다. <span><script type="math/tex">\log</script></span> 함수는 [단조증가](https://en.wikipedia.org/wiki/Monotonic_function) 함수이기 때문에, Likelihood를 최대로 만드는 모수와 Log-likelihood를 최대로 만드는 모수는 결국 같다는 사실을 이용한다. 
<div class="math"><script type="math/tex; mode=display">
\hat{\theta} = \underset{\theta}{\arg \max} \mathcal{L} (\theta; \mathbf{x}) = \underset{\theta}{\arg \max} \ln\mathcal{L} (\theta; \mathbf{x})
</script></div>

이 경우 모수의 추정치 <span><script type="math/tex">\hat\theta</script></span>는 다음 방정식으로 도출된다. 

<div class="math"><script type="math/tex; mode=display">
\frac{\partial}{\partial \theta} \ln\mathcal{L} (\theta; \mathbf{x}) \Big|_{\hat\theta} = \frac{\partial}{\partial \theta} \left[ \sum_{i=1}^n \ln \left[ \mathbf{F}(x_i ; \theta) \right]  \right]_{\hat\theta} = 0
</script></div>

<br/>

## 베르누이 분포의 모수추정
베르누이 분포로부터 독립적으로 추출된 <span><script type="math/tex">n</script></span>개의 샘플 <span><script type="math/tex">\mathbf{x} = (x_1, \cdots, x_n) \in \mathbb{R}^n</script></span> 가 있고, 총 성공횟수를 <span><script type="math/tex">n^* \equiv \sum^n_{i=1} x_i</script></span> 라고 가정해보자. 이 베르누이 분포의 모수(즉 성공확률) <span><script type="math/tex">\theta \in \mathbb{R}</script></span> 를 수식적으로 추정하기에 앞서, 우선 직관적으로 생각해보면 다음과 같을 것이다.  

<div class="math"><script type="math/tex; mode=display">
\hat{\theta} = \frac{n^*}{n}
</script></div>

일단 이 값을 마음 속에 두고, 이번에는 MLE를 통해 모수를 추정해보자. 모수 <span><script type="math/tex">\theta</script></span>의 베르누이 확률밀도함수, 

<div class="math"><script type="math/tex; mode=display">
\mathbf{Bern} (x; \theta) = \theta^{x} (1-\theta)^{1-x}
</script></div> 

에 대해서, Log-likelihood는 다음과 같이 전개된다. 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\ln \mathcal{L} (\theta; \mathbf{x})
&= \sum_{i=1}^n \ln \left[ \mathbf{Bern}(x_i ; \theta) \right] \\
&= \sum^n_{i=1} \left[ x_i \ln \theta + (1-x_i) \ln(1-\theta) \right] \\
&= \left( \sum^n_{i=1} x_i \right) \ln \theta + \left( n - \sum^n_{i=1} x_i \right) \ln(1-\theta) \\
&= n^* \ln \theta + (n - n^*) \ln (1 - \theta) \\\\
\frac{\partial}{\partial \theta} \ln \mathcal{L} (\theta; \mathbf{x}) \Big|_{\hat\theta}
&= \frac{n^*}{\hat\theta} - \frac{n - n^*}{1-\hat\theta} = 0 
\end{aligned}
</script></div>

<div class="math"><script type="math/tex; mode=display">
\therefore \hat{\theta} = \frac{n^*}{n}
</script></div>

즉 직관적인 추정값과 정확하게 일치한다. 이는 다른 분포에서도 마찬가지로 적용된다. 

<br/>

## 카테고리 분포의 모수추정
<span><script type="math/tex">k</script></span>-클래스 카테고리 분포에서 독립적으로 추출된 <span><script type="math/tex">n</script></span>개의 샘플 <span><script type="math/tex">\mathbf{x} = (\mathbf{x}_1, \cdots, \mathbf{x}_n)</script></span> 로부터 분포의 모수 <span><script type="math/tex">\boldsymbol{\theta} = (\theta_1, \cdots, \theta_k)</script></span>를 추정해보자. 각각의 샘플 <span><script type="math/tex">\mathbf{x}_i = (x_{i1}, \cdots, x_{ik}) \in \mathbb{R}^k</script></span> 에 대해서 카테고리 분포의 확률밀도함수는 다음과 같다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{Cat} (\mathbf{x}_i; \boldsymbol{\theta}) = \prod^k_{j=1} \theta_j^{x_{ij}}
</script></div>

그리고 다음의 두 가지 사실을 기억해두자.  

* 카테고리 <span><script type="math/tex">j</script></span>의 성공횟수 <span><script type="math/tex">n_j \equiv \sum^n_{i=1} x_{ij}</script></span>
* <span><script type="math/tex">\sum^k_{j=1} n_j = n</script></span>

이제 Log-likelihood를 전개하면, 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\ln \mathcal{L} (\boldsymbol{\theta}; \mathbf{x})
&= \sum_{i=1}^n \ln \left[ \mathbf{Cat}(\mathbf{x}_i ; \boldsymbol{\theta}) \right] \\
&= \sum^n_{i=1} \sum^k_{j=1} x_{ij} \ln \theta_j \\
&= \sum^k_{j=1} \ln \theta_j \left( \sum^n_{i=1} x_{ij} \right) \\
&= \sum^k_{j=1} n_j \ln \theta_j 
\end{aligned}
</script></div>

여기서 <span><script type="math/tex">\theta_1 + \cdots + \theta_k = 1</script></span> 이라는 제약조건이 있으므로, 단순히 <span><script type="math/tex">\boldsymbol{\theta}</script></span>에 대해 미분하는 것만으로 해를 구할 수는 없다. 이 경우에는 [라그랑제 승수법 (Lagrange multiplier)](https://en.wikipedia.org/wiki/Lagrange_multiplier)을 이용하여 최적해 <span><script type="math/tex">{\hat{\boldsymbol{\theta}}}</script></span>를 구한다. <span><script type="math/tex">\lambda \in \mathbb{R}</script></span> 에 대해서 라그랑제 함수 <span><script type="math/tex">\mathbf{L}</script></span>을 다음과 같이 정의하면, 

<div class="math"><script type="math/tex; mode=display">
\mathbf{L} (\boldsymbol{\theta}, \lambda) \equiv \ln \mathcal{L} (\boldsymbol{\theta}; \mathbf{x}) + \lambda \left( 1 - \sum_{j=1}^k \theta_j \right)
</script></div>

* <span><script type="math/tex">\frac{\partial}{\partial \lambda} \mathbf{L} \Big|_{\hat{\boldsymbol{\theta}}} = 0~~</script></span> <span><script type="math/tex">\Longrightarrow \sum_{j=1}^k \hat\theta_j = 1</script></span> (즉 제약조건과 동일)

* <span><script type="math/tex">\frac{\partial}{\partial \boldsymbol{\theta}} \mathbf{L} \Big|_{\hat{\boldsymbol{\theta}}} = 0</script></span> 에서, 


<div class="math"><script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_j} \mathbf{L} (\boldsymbol{\theta}, \lambda) \Big|_{\hat\theta_j} = \frac{n_j}{\hat\theta_j} - \lambda = 0  ~\Rightarrow~ n_j = \lambda \hat\theta_j 
</script></div>

<div class="math"><script type="math/tex; mode=display">
n = \sum^k_{j=1} n_j = \lambda \sum^k_{j=1} \hat\theta_j = \lambda 
</script></div>

<div class="math"><script type="math/tex; mode=display">
\therefore \hat{\theta}_j = \frac{n_j}{n}
</script></div>

<br/>

## 정규분포의 모수추정
가우시안 정규분포에서 독립적으로 추출된 <span><script type="math/tex">n</script></span>개의 샘플 <span><script type="math/tex">\mathbf{x} = (x_1, \cdots, x_n) \in \mathbb{R}^n</script></span> 로부터 분포의 모수 <span><script type="math/tex">\boldsymbol{\theta} = (\mu, \sigma^2)</script></span>을 추정해보자. 가우시안 정규분포의 확률밀도함수는 

<div class="math"><script type="math/tex; mode=display">
\mathcal{N}(x; \boldsymbol{\theta}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left[ - \frac{(x - \mu)^2}{2 \sigma^2} \right]
</script></div>

이므로, 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\ln \mathcal{L} (\boldsymbol{\theta}; \mathbf{x}) 
&= \sum_{i=1}^n \ln \left[ \mathcal{N}(x_i ; \boldsymbol{\theta}) \right] \\
&= \sum^n_{i=1} \left[ - \frac{1}{2} \ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} (x_i - \mu)^2 \right] \\
&= - \frac{n}{2} \ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \sum^n_{i=1} (x_i - \mu)^2
\end{aligned}
</script></div>


* <span><script type="math/tex">\frac{\partial}{\partial \mu} \ln \mathcal{L} \Big|_{\hat{\boldsymbol{\theta}}} = 0</script></span> 에서, 

<div class="math"><script type="math/tex; mode=display">
\frac{\partial}{\partial \mu} \ln \mathcal{L} (\boldsymbol{\theta}; \mathbf{x}) \Big|_{\hat{\boldsymbol{\theta}}} = \frac{1}{\sigma^2} \sum^n_{i=1} (x_i - \hat\mu) = 0
</script></div>

<div class="math"><script type="math/tex; mode=display">
\therefore \hat{\mu} = \frac{1}{n} \sum^n_{i=1} x_i
</script></div>


* <span><script type="math/tex">\frac{\partial}{\partial \sigma^2} \ln \mathcal{L} \Big|_{\hat{\boldsymbol{\theta}}} = 0</script></span> 에서, 

<div class="math"><script type="math/tex; mode=display">
\frac{\partial}{\partial \sigma^2} \ln \mathcal{L} (\theta; \mathbf{x}) \Big|_{\hat{\boldsymbol{\theta}}} = - \frac{n}{2 \hat\sigma^2} + \frac{1}{2 (\hat\sigma^2)^2} \sum^n_{i=1} (x_i - \hat\mu)^2 = 0
</script></div>

<div class="math"><script type="math/tex; mode=display">
\therefore \hat{\sigma}^2 = \frac{1}{n} \sum^n_{i=1} (x_i - \hat\mu)^2
</script></div>


<br/>

## 다변수 정규분포의 모수추정
다변수 가우시안 정규분포에서 독립적으로 추출된 <span><script type="math/tex">n</script></span>개의 샘플 <span><script type="math/tex">\mathbf{x} = (\mathbf{x}_1, \cdots, \mathbf{x}_n)</script></span>, <span><script type="math/tex">\mathbf{x}_i \in \mathbb{R}^d</script></span>로부터 분포의 모수 <span><script type="math/tex">{\Theta} = ({\mu}, \mathbf{\Sigma})</script></span>을 추정해보자. 각각의 샘플 <span><script type="math/tex">\mathbf{x}_i</script></span>에 대해 다변수 가우시안 정규분포의 확률밀도함수는 

<div class="math"><script type="math/tex; mode=display">
\mathbf{N}_d (\mathbf{x}_i; \Theta) = \frac{1}{(2 \pi)^{d/2} |\mathbf{\Sigma}|^{1/2}} \exp \left[ -\frac{1}{2} (\mathbf{x}-{\mu})^\mathsf{T} \mathbf{\Sigma}^{-1} (\mathbf{x}-{\mu}) \right]
</script></div>

이므로, 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\ln \mathcal{L} ({\Theta}; \mathbf{x}) 
&= \sum^n_{i=1} \ln \left[ \mathbf{N}_d (\mathbf{x}_i ; \Theta)  \right]\\
&= \sum^n_{i=1} \left[ - {d \over 2} \ln (2 \pi) - {1 \over 2} \ln |\mathbf{\Sigma}| - {1 \over 2} (\mathbf{x}_i-{\mu})^\mathsf{T} \mathbf{\Sigma}^{-1} (\mathbf{x}_i-{\mu}) \right] \\
&= - {nd \over 2} \ln (2 \pi) - {n \over 2} \ln |\mathbf{\Sigma}| - {1 \over 2} \sum^n_{i=1} (\mathbf{x}_i-{\mu})^\mathsf{T} \mathbf{\Sigma}^{-1} (\mathbf{x}_i-{\mu}) \\
&= - {nd \over 2} \ln (2 \pi) + {n \over 2} \ln |\mathbf{\Lambda}| - {1 \over 2} \sum^n_{i=1} (\mathbf{x}_i-{\mu})^\mathsf{T} \mathbf{\Lambda} (\mathbf{x}_i-{\mu})
\end{aligned}
</script></div>


여기서 <span><script type="math/tex">\mathbf{\Sigma}^{-1} \overset{\text{let}}{=} \mathbf{\Lambda}</script></span> ([Precision 행렬](https://en.wikipedia.org/wiki/Precision_(statistics))이라고 한다)로 치환하였다. 이제 몇몇 유용한 공식들을 소환하자. [여기](https://gem763.github.io/linear%20algebra/%ED%96%89%EB%A0%AC%EC%9D%98-%EB%AF%B8%EB%B6%84.html#%ED%96%89%EB%A0%AC%EB%AF%B8%EB%B6%84%EC%9D%98-%EC%A3%BC%EC%9A%94%EC%84%B1%EC%A7%88) 참고. 

* <span><script type="math/tex">|\mathbf{\Sigma}| = |\mathbf{\Lambda}^{-1}| = |\mathbf{\Lambda}|^{-1}</script></span>
* <span><script type="math/tex">\frac{\partial}{\partial \mathbf{\Lambda}} \ln |\mathbf{\Lambda}| = \mathbf{\Lambda}^{-\mathsf{T}} = \mathbf{\Sigma}^\mathsf{T} = \mathbf{\Sigma}</script></span>
* <span><script type="math/tex">\frac{\partial}{\partial \mathbf{\Lambda}} \operatorname{tr} \left( (\mathbf{x}_i - {\mu})^\mathsf{T} \mathbf{\Lambda} (\mathbf{x}_i - {\mu}) \right)</script></span> <span><script type="math/tex">= \frac{\partial}{\partial \mathbf{\Lambda}} \operatorname{tr} \left( (\mathbf{x}_i - {\mu}) (\mathbf{x}_i - {\mu})^\mathsf{T} \mathbf{\Lambda} \right)</script></span> <span><script type="math/tex">= (\mathbf{x}_i - {\mu}) (\mathbf{x}_i - {\mu})^\mathsf{T}</script></span>

이 공식들을 이용하면, 

* <span><script type="math/tex">\frac{\partial}{\partial {\mu}} \ln \mathcal{L} \Big|_{\hat\Theta} = 0</script></span> 에서, 

<div class="math"><script type="math/tex; mode=display">
\frac{\partial}{\partial {\mu}} \ln \mathcal{L} ({\Theta}; \mathbf{x}) \Big|_{\hat\Theta} = \hat{\mathbf{\Lambda}}^\mathsf{T} \sum^n_{i=1} (\mathbf{x}_i - {\hat\mu}) = 0 
</script></div>

<div class="math"><script type="math/tex; mode=display">
\therefore \hat\mu = \frac{1}{n} \sum^n_{i=1} \mathbf{x}_i
</script></div>

* <span><script type="math/tex">\frac{\partial}{\partial \mathbf{\Lambda}} \ln \mathcal{L} \Big|_{\hat\Theta} = 0</script></span> 에서, 

<div class="math"><script type="math/tex; mode=display">
\frac{\partial}{\partial \mathbf{\Lambda}} \ln \mathcal{L} ({\Theta}; \mathbf{x}) \Big|_{\hat\Theta} = \frac{n}{2} \hat{\mathbf{\Sigma}} - \frac{1}{2} \sum^n_{i=1} (\mathbf{x}_i - {\hat\mu}) (\mathbf{x}_i - {\hat\mu})^\mathsf{T} = 0 
</script></div>

<div class="math"><script type="math/tex; mode=display">
\therefore \hat{\mathbf{\Sigma}}= \frac{1}{n} \sum^n_{i=1} (\mathbf{x}_i - \hat\mu) (\mathbf{x}_i - \hat\mu)^\mathsf{T}
</script></div>

