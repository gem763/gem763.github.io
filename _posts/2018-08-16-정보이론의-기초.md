---
layout: post
title: 정보이론의 기초
tags: [Machine learning]
categories: [Machine learning]
excerpt_separator: <!--more-->

---

정보이론의 기초적인 문법을 정리한다. 정보의 의미와 엔트로피의 정의, KL 다이버전스 등에 대해 살펴본다. 차후에 다루게 될 머신러닝 관련 포스트에서, 수식전개를 돕기 위해 쓰이게 될 것이다. 
<!--more-->

* TOC
{:toc}


## 정보
정보는 **갖고 싶을 만한 가치**를 지니고 있어야 한다. 마음만 먹으면 누구라도 가질 수 있거나, 너무 자주 발생해서 쉽게 알아낼 수 있다면, 그건 좋은 정보라고 말할 수 없을 것이다. 혹자는 정보를 **놀람의 크기**(Surprising degree) 또는 **파급력** 정도로 이해하기도 한다. 예를들어 `내일은 추울수도 있고 더울수도 있다`라는 식의 당연한 얘기는, 정보로서의 효용이 크다고 말하기는 힘들다. 하지만 `내일부터 장마가 시작된다`라는 일기예보는 누가 듣더라도 가치있는 정보가 된다. 

[정보이론(Information theory)](https://en.wikipedia.org/wiki/Information_theory)에 따르면 정보량 <span><script type="math/tex">\mathbf{I}(\cdot): \mathbb{R} \mapsto \mathbb{R}</script></span> 은 발생확률 <span><script type="math/tex">p</script></span> <span><script type="math/tex">(0 \le p \le 1)</script></span> 의 함수이며, 다음의 4가지 성질을 지니고 있다고 한다. 

* 발생확률이 작을 수록 더 많은 정보량을 갖고 있다: 

<div class="math"><script type="math/tex; mode=display">
p_i \ge p_j \Longrightarrow \mathbf{I}(p_i) \le \mathbf{I}(p_j)
</script></div>

* 정보량은 음이 아니다:
<div class="math"><script type="math/tex; mode=display">
\mathbf{I}(p) \ge 0
</script></div>

* 반드시 발생하는 사건(deterministic event)의 정보량은 0이다. (즉 정보가치가 없다)
<div class="math"><script type="math/tex; mode=display">\mathbf{I}(1) = 0</script></div>

* 독립인 사건들의 정보량은 단순합산(additive)할 수 있다. 
<div class="math"><script type="math/tex; mode=display">
\mathbf{I}(p_i p_j) = \mathbf{I}(p_i) + \mathbf{I}(p_j)
</script></div>

여기서 정보량 <span><script type="math/tex">\mathbf{I}</script></span> 는 [**self-information**](https://en.wikipedia.org/wiki/Self-information) 또는 **surprisal** 이라고도 부른다. 섀넌은 위의 성질들을 모두 만족하는 **로그함수**를 이용하여, 다음과 같이 정보량을 정의하였다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{I}(p) \equiv \log (1/p) = -\log p
</script></div>


정보이론에서는 기본 연산단위가 비트(bit)이기 때문에, 주로 이진로그(<span><script type="math/tex">\log_2</script></span>)로 정보량을 정의한다. 이 때의 정보량 단위를  **비트**(bit) 또는 **섀넌**(shannon)이라고 한다. 예를들어 발생확률이 25%와 50%인 정보의 정보량을 각각 계산해보면, 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{I}(0.25) &= \mathbf{I}(2^{-2})= - \log_2 2^{-2} = 2 ~\text{bit} \\
\mathbf{I}(0.50) &= \mathbf{I}(2^{-1})= - \log_2 2^{-1} = 1 ~\text{bit}
\end{aligned}
</script></div>

즉 발생확률이 낮을 수록 정보량이 더 크다. 참고로 **머신러닝**에서는 자연로그(<span><script type="math/tex">\ln</script></span>)로 정보량을 정의하는 경우가 많은데, <span><script type="math/tex">\ln x</script></span>가 지수함수 <span><script type="math/tex">e^x</script></span>의 역함수일 뿐만 아니라 간편한 미분연산 등 대수적인 전개가 용이하기 때문이다.  이처럼 자연로그를 쓰는 경우의 정보량 단위를 **내트**(nat) 라고 한다. 정보량의 정의에 의해, <span><script type="math/tex">0 \le \mathbf{I}(p) \lt \infty</script></span> 임을 알 수 있다. 


<center><img src="https://gem763.github.io/assets/img/20180816/info_shape.PNG" alt="info_shape"/></center>

<br/>

## 엔트로피

[**엔트로피**(Entropy)](https://en.wikipedia.org/wiki/Entropy_(information_theory))는 **확률변수의 불확실성(Uncertainty)을 측정**하는 도구 중의 하나이다. 원래는 고전 열역학에서 소개된 개념인데, 섀넌의 1948년 논문인 [A Mathematical Theory of Communication](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)을 통해 정보이론으로 접목되었다. 확률변수 <span><script type="math/tex">X</script></span>에 대하여, **엔트로피** <span><script type="math/tex">\mathbf{H} \in \mathbb{R}</script></span>는 **정보량** <span><script type="math/tex">\mathbf{I}</script></span> **의 기대값**으로 정의된다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[X] \equiv  \mathbf{E} [\mathbf{I}(p(X))] = -\mathbf{E}[\log p(X)] = \mathbf{H}(p)
</script></div>

여기서 <span><script type="math/tex">p(x)</script></span>는 확률변수 <span><script type="math/tex">X</script></span>의 확률밀도함수를 의미하며, 엔트로피를 <span><script type="math/tex">p</script></span>의 함수로 이해하는 경우도 있기 때문에, 위의 정의처럼 <span><script type="math/tex">\mathbf{H}(p)</script></span>라고 표기하기도 한다. 한편 엔트로피의 단위는 정보량의 단위에 따라 달라진다. [^unit_entropy]

[^unit_entropy]: 만약 정보량의 단위가 비트라면(즉 정보량을 이진로그로 정의한다면) 엔트로피의 단위도 비트가 된다. 반대로 정보량의 단위가 내트라면(즉 정보량을 자연로그로 정의한다면) 엔트로피의 단위 역시 내트가 된다. 

엔트로피는 **정보의 불확실성**이나, 해당 정보를 접했을 때 느끼는 **평균적인 파급력(놀람)의 크기** 정도로 해석한다. 왜냐하면 다음과 같은 추론이 가능하기 때문이다. 

1. 정보량의 기대값이 크면, 
2. 해당 정보에 대한 평균적인 파급력의 크기가 클 것이고, 
3. 이느 결과적으로 해당 정보의 불확실성이 크다는 것을 의미한다. 

만약 확률변수 <span><script type="math/tex">X</script></span>가 [베르누이 분포](https://gem763.github.io/probability%20theory/%EB%B2%A0%EB%A5%B4%EB%88%84%EC%9D%B4-%EB%B6%84%ED%8F%AC%EC%99%80-%EC%9D%B4%ED%95%AD%EB%B6%84%ED%8F%AC.html) 또는 [카테고리 분포](https://gem763.github.io/probability%20theory/%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC-%EB%B6%84%ED%8F%AC%EC%99%80-%EB%8B%A4%ED%95%AD%EB%B6%84%ED%8F%AC.html) 등의 **이산확률분포**를 따른다면, 엔트로피를 좀 더 명시적으로 나타낼 수 있다. 이를 **섀넌 엔트로피**(Shannon entropy) 또는 **정보 엔트로피**(Information entropy) 라고 부르기도 한다.[^info_entropy] 이산확률변수 <span><script type="math/tex">X</script></span>의 표본공간(Sample space)[^sample_space]을 <span><script type="math/tex">\mathbb{X}</script></span> 라고 하면, 섀넌 엔트로피는 다음과 같다. 

[^sample_space]: 확률변수 <span><script type="math/tex">X</script></span>가 취할 수 있는 모든 값의 범위를 뜻한다. [여기](https://en.wikipedia.org/wiki/Sample_space)를 참고. 

[^info_entropy]: 이와는 반대 개념으로, 연속확률변수에 대한 엔트로피를 [연속 엔트로피(Continuous entropy or Differential entropy)](https://en.wikipedia.org/wiki/Entropy_(information_theory)#Extending_discrete_entropy_to_the_continuous_case) 라고 하며, 다음과 같이 정의된다. <script type="math/tex; mode=display">\mathbf{H}[X] = -\int_{x \in \mathbb{X}} p(x) \log p(x) ~dx</script>

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[X] = -\sum_{x \in \mathbb{X}} p(x) \log p(x)
</script></div>

이 때 <span><script type="math/tex">p(\cdot)</script></span> 는 확률질량함수가 된다. 

---

Notation이 다소 헷갈릴 수도 있으니 정리하고 넘어가자. 다음의 설정들에 대해서, 
* 이산확률변수 <span><script type="math/tex">X</script></span>의 표본공간 <span><script type="math/tex">\mathbb{X} = \{ x_1, \cdots, x_n \}</script></span>
* <span><script type="math/tex">X</script></span>의 각 샘플 <span><script type="math/tex">x_i</script></span>에 할당된 확률값 <span><script type="math/tex">p_i \equiv p(x_i)</script></span>
* <span><script type="math/tex">p_1 + \cdots + p_n = 1</script></span>


아래의 엔트로피 표현들은 모두 같은 의미를 지닌다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[X] = \mathbf{H}(p) = \mathbf{H}_n (p_1, \cdots, p_n) = - \sum_{i=1}^n p_i \log p_i
</script></div>

표본공간의 크기가 <span><script type="math/tex">n</script></span> 이라는 것을 표현하기 위해, 각 확률값 <span><script type="math/tex">p_i</script></span>의 함수로 나타내는 경우, <span><script type="math/tex">\mathbf{H}_n(\cdots)</script></span> 과 같이 <span><script type="math/tex">n</script></span>을 명시적으로 표시하였다. 

<br/>

> <big><b><span><script type="math/tex">p = 0</script></span> 인 경우</b></big>
> 
> 이산확률변수 <span><script type="math/tex">X</script></span>에서 추출된 어떤 샘플의 발생확률이 <span><script type="math/tex">p=0</script></span> 일 때에는, 해당 엔트로피의 계산에 <span><script type="math/tex">0 \log 0</script></span> 이 포함되기 때문에, 엔트로피가 명확히 정의되지 않는다. 이 경우에는 다음을 정의하여 이용한다. 
> <div class="math"><script type="math/tex; mode=display">\Bigl[ p \log p \Bigr]_{p = 0} \equiv \lim_{p \to 0+} p \log p</script></div>
>[로피탈의 정리(L'Hospital's rule)](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule)에 의해, 
><div class="math"><script type="math/tex; mode=display">\lim_{p \to 0+} p \log p = \lim_{p \to 0+} \frac{(\log p)'}{(1/p)'} = \lim_{p \to 0+} \frac{1/p}{-1/p^2} = 0</script></div>
>이므로, 발생확률이 0인 정보량은 엔트로피 계산에서 제외해도 상관없게 된다. 따라서 발생확률이 0이 아닌 표본공간 <span><script type="math/tex">\mathbb{X}_o</script></span> <span><script type="math/tex">(\subset \mathbb{X})</script></span>에 대해서, 엔트로피는 다음과 같이 계산된다. 
><div class="math"><script type="math/tex; mode=display">\mathbf{H}[X] = -\sum_{x \in \mathbb{X}_o} p(x) \log p(x)</script></div>
>



<br/>

### 예시: 동전 던지기
동전 던지기를 통해 엔트로피의 개념을 이해해보자. 동전 던지기의 확률변수 <span><script type="math/tex">X \in \{ 0, 1 \}</script></span>는 성공확률 <span><script type="math/tex">\theta \equiv \Pr[X=1] \in \mathbb{R}</script></span> 의 [베르누이 분포](https://gem763.github.io/probability%20theory/%EB%B2%A0%EB%A5%B4%EB%88%84%EC%9D%B4-%EB%B6%84%ED%8F%AC%EC%99%80-%EC%9D%B4%ED%95%AD%EB%B6%84%ED%8F%AC.html#%EB%B2%A0%EB%A5%B4%EB%88%84%EC%9D%B4-%EB%B6%84%ED%8F%AC)를 따른다. 확률질량함수 <span><script type="math/tex">p(x) = \mathbf{Bern}(x;\theta)</script></span> 에 대하여, 자연로그로 정의된 엔트로피를 산출해보면, 

<div class="math"><script type="math/tex; mode=display">
X \sim \mathbf{Bern}(\theta)
</script></div>

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}[X] &= -p(1) \ln p(1) - p(0) \ln p(0)\\
&= -\theta \ln \theta - (1-\theta) \ln (1-\theta)
\end{aligned}
</script></div>

이와 같이, 베르누이 분포의 엔트로피를 성공확률 <span><script type="math/tex">\theta</script></span>의 함수로 나타내는 것을 [이진 엔트로피 함수 (Binary entropy function)](https://en.wikipedia.org/wiki/Binary_entropy_function) 라고 부른다. [^bin_entropy]

[^bin_entropy]: 보통은 이진로그로 정의하는 경우가 많으나, 여기에서는 자연로그를 썼다. 


* <span><script type="math/tex">\theta=0</script></span> 인 경우: 
<div class="math"><script type="math/tex; mode=display">\mathbf{H}[X] = -0 \ln 0 - 1 \ln 1 = 0</script></div>

* <span><script type="math/tex">\theta=0.3</script></span> 인 경우: 
<div class="math"><script type="math/tex; mode=display">\mathbf{H}[X] = -0.3 \ln 0.3 - 0.7 \ln 0.7 \approx 0.61</script></div>

* <span><script type="math/tex">\theta=0.5</script></span> 인 경우: 
<div class="math"><script type="math/tex; mode=display">\mathbf{H}[X] = -0.5 \ln 0.5 - 0.5 \ln 0.5 \approx 0.69</script></div>

따라서 <span><script type="math/tex">\theta=0.5</script></span> 에 가까울 수록 엔트로피가 커진다는 사실을 알 수 있다. **공정(fair)한 동전일 수록 불확실성이 커진다**는 것을 의미하는데, 사실 이는 동전 던지기의 확률분포 뿐만이 아니라 다른 모든 확률분포에 대해서도 마찬가지로 적용이 된다. 0과 1 사이의 모든 <span><script type="math/tex">\theta</script></span>에 대해서 엔트로피를 그려보면 다음 차트를 얻는다. 

<center><img src="https://gem763.github.io/assets/img/20180816/entropy_bern.PNG" alt="entropy_bern"/></center>

<br/>


### 주요성질
확률변수 <span><script type="math/tex">X \in \mathbb{X}</script></span> 와 <span><script type="math/tex">Y \in \mathbb{Y}</script></span> 가 다음과 같이 정의된 이산확률분포를 따른다고 가정해보자. 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbb{X} = \{ x_1, \cdots, x_n \}, ~p_i = \Pr[X=x_i] \\
\mathbb{Y} = \{ y_1, \cdots, y_m \}, ~q_j = \Pr[Y=y_j]
\end{aligned}
</script></div>

<br/>

#### Zero probability의 기여
확률이 0인 샘플은 엔트로피에 전혀 영향을 주지 못한다. <span><script type="math/tex">0 \log 0 = 0</script></span> 으로 정의되기 때문이다. 위에서 [<span><script type="math/tex">p=0</script></span> 인 경우]를 참고. 
<div class="math"><script type="math/tex; mode=display">
\mathbf{H}_{n+1}(p_1, \cdots, p_n, 0) = \mathbf{H}_{n}(p_1, \cdots, p_n)
</script></div>

<br/>

#### 엔로피의 최대값
[이산균등분포 (Discrete uniform distribution)](https://en.wikipedia.org/wiki/Discrete_uniform_distribution)일때 엔트로피가 최대값을 가진다. 
<div class="math"><script type="math/tex; mode=display">
\mathbf{H}_{n}(p_1, \cdots, p_n) \le \mathbf{H}_n (\tfrac{1}{n}, \cdots, \tfrac{1}{n}) = \log n
</script></div>

**Proof.**
<span><script type="math/tex">\log</script></span>는 [Strictly concave 함수](https://gem763.github.io/machine%20learning/Affinity%EC%99%80-Convexity.html#convex-function)이므로, [Jensen 부등식](https://gem763.github.io/machine%20learning/Affinity%EC%99%80-Convexity.html#%ED%99%95%EB%A5%A0%EB%B3%80%EC%88%98%EC%9D%98-jensen-%EB%B6%80%EB%93%B1%EC%8B%9D)에 의해 다음을 알 수 있다. 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}_n (p_1, \cdots, p_n) 
&= \mathbf{H}[X] \\
&= -\mathbf{E}[\log p(X)] \\
&= \mathbf{E} \left[\log \left( \frac{1}{p(X)} \right) \right] \\
&\le \log \mathbf{E} \left[\frac{1}{p(X)} \right]
\end{aligned}
</script></div>

여기서 등호는 <span><script type="math/tex">\frac{1}{p(X)}</script></span>가 상수일 때, 즉 <span><script type="math/tex">p_1 = \cdots = p_n = \frac{1}{n}</script></span> 인 경우에만 발생하므로, 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}_n(\tfrac{1}{n}, \cdots, \tfrac{1}{n}) = \log \mathbf{E} \left[\frac{1}{p(X)} \right] = \log n
\end{aligned}
</script></div>

참고로, 연속확률분포의 (연속) 엔트로피가 최대가 되려면, 해당 확률분포가 가우시안 정규분포를 따라야 한다는 사실이 알려져있다. [여기](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution)를 참고. 


<br/>

#### 독립분포의 엔트로피 가산성
<span><script type="math/tex">X</script></span>와 <span><script type="math/tex">Y</script></span>가 서로 독립적인 이산확률분포를 따른다면, 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[X,Y] = \mathbf{H}[X] + \mathbf{H}[Y]
</script></div>

이는, **독립적인 불확실성은 가산된다**는 의미로 이해할 수 있다. 

**Proof.**

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}[X,Y] 
&= \mathbf{H}_{nm}(p_1 q_1, \cdots,  p_i q_j, \cdots, p_n q_m) \\
&= -\sum_{i,j} p_i q_j \log (p_i q_j) \\
&= -\sum_{i=1}^n\sum_{j=1}^m p_i q_j (\log p_i + \log q_j) \\
&= -\sum_{i=1}^n\sum_{j=1}^m (p_i q_j \log p_i + p_i q_j\log q_j) \\
&= -\sum_{j=1}^m q_j \left(\sum_{i=1}^n p_i \log p_i \right) - \sum_{i=1}^n p_i \left( \sum_{j=1}^m q_j\log q_j \right) \\
&= \mathbf{H}_n(p_1, \cdots, p_n) + \mathbf{H}_m(q_1, \cdots, q_m) \\
&= \mathbf{H}[X] + \mathbf{H}[Y]
\end{aligned}
</script></div>


<br/>

#### 균등분포의 엔트로피
균등분포의 경우, 표본공간의 크기가 클 수록 (즉 확률변수가 취할 수 있는 값의 수가 많을 수록) 엔트로피가 커진다. 
<div class="math"><script type="math/tex; mode=display">
\mathbf{H}_n (\tfrac{1}{n}, \cdots, \tfrac{1}{n}) \le \mathbf{H}_{n+1} (\tfrac{1}{n+1}, \cdots, \tfrac{1}{n+1})
</script></div>

이를테면 동전 던지기 보다 주사위 던지기의 엔트로피가 더 크다고 할 수 있다. 

**Proof.**
<div class="math"><script type="math/tex; mode=display">
\mathbf{H}_n (\tfrac{1}{n}, \cdots, \tfrac{1}{n}) = \log n \le \log (n+1) = \mathbf{H}_{n+1} (\tfrac{1}{n+1}, \cdots, \tfrac{1}{n+1})
</script></div>


<br/>

## 결합 엔트로피
[**결합 엔트로피** (Joint entropy)](https://en.wikipedia.org/wiki/Information_theory#Joint_entropy)는 [결합확률분포 (Joint probability distribution)](https://en.wikipedia.org/wiki/Joint_probability_distribution)의 엔트로피를 말한다. 
두 확률변수 <span><script type="math/tex">X, Y</script></span>의 결합확률변수 <span><script type="math/tex">(X,Y)</script></span>에 대한 결합 엔트로피는 다음과 같이 정의된다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[X,Y] \equiv -\mathbf{E} [\log p(X,Y)]
</script></div>

여기서 <span><script type="math/tex">p(x,y)</script></span>는 결합확률변수의 임의의 샘플 <span><script type="math/tex">(x,y)</script></span>에 대한 확률밀도함수를 의미한다. 확장하여, 결합확률변수 <span><script type="math/tex">\mathbf{X} = (X_1, \cdots, X_n)</script></span>의 샘플 <span><script type="math/tex">\mathbf{x} = (x_1, \cdots, x_n)</script></span>에 대한 확률밀도함수를 <span><script type="math/tex">p(\mathbf{x})</script></span> 라고 하면, 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[\mathbf{X}] \equiv -\mathbf{E} [\log p(\mathbf{X})]
</script></div>

가 된다. 만약 각 확률변수 <span><script type="math/tex">X_i</script></span>가 모두 이산확률분포를 따른다면, <span><script type="math/tex">\mathbf{x}</script></span>가 취할 수 있는 모든 값에 대하여 다음과 같이 나타낼 수 있다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[\mathbf{X}] = - \sum_{\mathbf{x}} p(\mathbf{x}) \log p(\mathbf{x})
</script></div>

<br/>

## 교차 엔트로피
하나의 확률변수 <span><script type="math/tex">X</script></span>를 묘사하는 두 개의 확률분포 <span><script type="math/tex">p</script></span> 와 <span><script type="math/tex">q</script></span> 가 있다고 생각해보자. 현실에서는 이런 경우가 빈번하게 발생한다. 이를테면 확률변수 <span><script type="math/tex">X</script></span>의 **분포를 모르고 있는 상태**에서 확률밀도함수를 추정한다면, 해당 확률밀도함수의 추정된 형태는 여러가지가 될 수 있는 것이다. 이럴 때에는 다음과 같이 [**교차 엔트로피** (Cross entropy)](https://en.wikipedia.org/wiki/Cross_entropy) <span><script type="math/tex">\mathbf{H}(p,q)</script></span> 를 정의할 수 있다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}(p,q) \equiv - \mathbf{E}_{p} [\log q(X)]
</script></div>

여기서 <span><script type="math/tex">\mathbf{E}_p[\cdot]</script></span>는 <span><script type="math/tex">X</script></span>의 확률밀도함수가 <span><script type="math/tex">p</script></span> 인 경우의 기대값을 의미한다. 로그 안밖의 확률밀도함수가 다르다는 점만 제외하고는, 기존의 엔트로피 정의와 거의 동일하다. 만약 확률변수 <span><script type="math/tex">X</script></span>가 이산확률분포를 따른다면, <span><script type="math/tex">X</script></span>의 표본공간 <span><script type="math/tex">\mathbb{X}</script></span>에 대해 다음과 같이 나타낼 수 있다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}(p,q) = - \sum_{x \in \mathbb{X}} p(x) \log q(x)
</script></div>

베르누이 분포를 예로 들어보자. 확률변수 <span><script type="math/tex">X</script></span>의 세 확률분포 <span><script type="math/tex">p</script></span>, <span><script type="math/tex">q_1</script></span>, <span><script type="math/tex">q_2</script></span>에 대한 성공확률 <span><script type="math/tex">\theta</script></span>를 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\theta_p &\equiv p(1) = 0.2 \\
\theta_{q_1} &\equiv q_1(1) = 0.3 \\
\theta_{q_2} &\equiv q_2(1) = 0.9
\end{aligned}
</script></div>

라고 한다면, 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}(p,q_1) &= -\theta_{p} \ln \theta_{q_1} - (1-\theta_{p}) \ln (1-\theta_{q_1}) \\
&= - (0.2 \times \ln 0.3) - (0.8 \times \ln 0.7) \\
&= 0.526
\end{aligned}
</script></div>

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}(p,q_2) &= -\theta_{p} \ln \theta_{q_2} - (1-\theta_{p}) \ln (1-\theta_{q_2}) \\
&= - (0.2 \times \ln 0.9) - (0.8 \times \ln 0.1) \\
&= 1.863
\end{aligned}
</script></div>

**두 확률분포가 유사할 수록 교차 엔트로피가 작아진다**는 사실을 알 수 있다. 베르누이 분포에서 0과 1 사이의 모든 성공확률 <span><script type="math/tex">\theta_p</script></span>, <span><script type="math/tex">\theta_q</script></span>에 대한 교차 엔트로피를 그려보면 다음 차트를 얻는다. 두 확률분포가 유사한 구간인 <span><script type="math/tex">\theta_p \approx \theta_q</script></span> 부근(점선)에서 교차 엔트로피가 0에 가깝다는 것을 다시한번 확인할 수 있다. 

<center><img src="https://gem763.github.io/assets/img/20180816/cross_entropy.PNG" alt="cross_entropy"/></center>  

이와 같은 성질 때문에, 교차 엔트로피는 머신러닝의 [분류 (Classification)](https://en.wikipedia.org/wiki/Statistical_classification) 문제에서 [비용함수 (Cost function)](https://en.wikipedia.org/wiki/Loss_function)으로 쓰이는 경우가 많다. 위의 예를 다시한번 가져와 보자. <span><script type="math/tex">p</script></span>를 <span><script type="math/tex">X</script></span>의 실제 확률분포라고 하고, 해당 확률분포를 추정하여 <span><script type="math/tex">q_1</script></span>과 <span><script type="math/tex">q_2</script></span>를 얻게 되었다고 하자. 각 성공확률값에 따라 [One-hot 인코딩](https://en.wikipedia.org/wiki/One-hot)을 통해 클래스를 분류해보면, 


<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
p : (0.2, 0.8) &\xrightarrow{\text{One-hot }} p': (0, 1) = \color{red}{\text{class 2}}\\
q_1 : (0.3, 0.7) &\xrightarrow{\phantom{\text{One-hot }}} q_1': (0, 1) = \text{class 2}\\
q_2 : (0.9, 0.1) &\xrightarrow{\phantom{\text{One-hot }}} q_2': (1, 0) = \text{class 1}
\end{aligned}
</script></div>

여기서 <span><script type="math/tex">p', q_1', q_2'</script></span>는 One-hot 인코딩 과정을 통해 예상되는 클래스를 의미한다. <span><script type="math/tex">p</script></span>가 실제 분포라고 했으므로, 이 확률변수를 분류해보면 **class 2**가 틀림없을 것이다. 이제, 추정된 분포 <span><script type="math/tex">q_1</script></span>과 <span><script type="math/tex">q_2</script></span>를 통해 인코딩된 분류를, 교차 엔트로피를 이용하여 검증해보자. 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}(p',q_1') &= -0 \ln 0 - 1 \ln 1 = 0 \\
\mathbf{H}(p',q_2') &= -0 \ln 1 - 1 \ln 0 = \infty \\
\end{aligned}
</script></div>

즉, 잘못된 분류를 도출하는 분포의 경우에는 교차 엔트로피가 무한대로 발산하게 된다. 


<br/>

## 조건부 엔트로피
두 확률변수 <span><script type="math/tex">X, Y</script></span>가 서로 상관관계가 있을 때, 확률변수 <span><script type="math/tex">X</script></span>의 실현값 <span><script type="math/tex">x</script></span>를 조건으로 확률변수 <span><script type="math/tex">Y</script></span>의 엔트로피를 구해보면 <span><script type="math/tex">\mathbf{H}[Y|X=x]</script></span> 가 된다. 이 값의 <span><script type="math/tex">X</script></span>에 대한 기대값을 [**조건부 엔트로피** (Conditional entropy)](https://en.wikipedia.org/wiki/Conditional_entropy) [^cond_entropy]라고 하며, 다음과 같이 정의된다. 

[^cond_entropy]: 조건부 불확실성 또는 Equivocation 라고도 부른다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[Y | X] 
\equiv \mathbf{E}_{X} \bigl[ \mathbf{H}[Y | X=x] \bigr]
</script></div>

여기서 <span><script type="math/tex">\mathbf{E}_X [\cdot]</script></span>은 확률변수 <span><script type="math/tex">X</script></span>에 대한 기대값을 의미한다. 만약 확률변수 <span><script type="math/tex">X</script></span>, <span><script type="math/tex">Y</script></span>가 이산확률분포를 따른다면, [조건부 확률](https://en.wikipedia.org/wiki/Conditional_probability) 및 엔트로피의 정의에 의해 다음과 같이 전개할 수 있다. 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}[Y | X] 
&= \sum_x p(x) ~\mathbf{H}[Y | x] \\
&= -\sum_x p(x) ~\sum_y p(y | x) \log p(y | x)\\
&= -\sum_{x,y} p(x) ~p(y | x) \log p(y | x)\\
&= -\sum_{x,y} p(x,y)  \log p(y | x)
\end{aligned}
</script></div>

여기서 <span><script type="math/tex">p(x)</script></span>, <span><script type="math/tex">p(y|x)</script></span>, <span><script type="math/tex">p(x,y)</script></span> 는 각각 확률변수 <span><script type="math/tex">X</script></span>, <span><script type="math/tex">Y|X</script></span>, <span><script type="math/tex">(X,Y)</script></span>의 확률질량함수를 의미한다. 결합 엔트로피 <span><script type="math/tex">\mathbf{H}[X,Y]</script></span> <span><script type="math/tex">= -\mathbf{E} [\log p(X,Y)]</script></span> <span><script type="math/tex">= -\sum_{x,y} p(x,y)  \log p(x,y)</script></span> 와 헷갈릴 수 있으므로, 주의하기 바란다. 

<br/>

### Chain rule
조건부 엔트로피는 다음과 같은 재미있는 성질이 있는데, 이를 **Chain rule** [^chain]이라고 부른다. 

[^chain]: 수학에는 굉장히 다양한 종류의 Chain rule 들이 존재한다. [여기](https://en.wikipedia.org/wiki/Chain_rule)와 [여기](https://en.wikipedia.org/wiki/Chain_rule_(probability))를 참고. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[Y | X] = \mathbf{H}[X,Y] - \mathbf{H}[X]
</script></div>


증명해보자. 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{H}[Y | X] 
&= -\sum_{x,y} p(x,y)  \log p(y | x) \\
&= -\sum_{x,y} p(x,y)  \log \frac{p(x,y)}{p(x)} \\
&= -\sum_{x,y} p(x,y) \log p(x,y) + \sum_{x,y} p(x,y) \log p(x)
\end{aligned}
</script></div>

여기서 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
-\sum_{x,y} p(x,y) \log p(x,y) &= \mathbf{H}[X,Y] \\
\sum_{x,y} p(x,y) \log p(x) &= \sum_x \left( \sum_y p(x,y) \right) \log p(x) \\
&= \sum_x p(x) \log p(x) \\
&= -\mathbf{H}[X]
\end{aligned}
</script></div>

임을 이용하면 증명이 완성된다. Chain rule을 활용하면 다음을 추가적으로 알 수 있다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}[X,Y] = \mathbf{H}[Y | X] + \mathbf{H}[X] = \mathbf{H}[X | Y] + \mathbf{H}[Y]
</script></div>



<br/>

> <big><b>엔트로피 Chain rule의 일반형</b></big>
> 
>Chain rule을 보다 일반적으로 기술하면 다음과 같다. 
>
><div class="math"><script type="math/tex; mode=display">
>\begin{aligned}
>\mathbf{H}[X_1, \cdots, X_n] 
>&= \sum_{i=1}^n \mathbf{H}[X_i \mid X_1, \cdots, X_{i-1}] 
>\end{aligned}
></script></div>
>
>증명은 간단하다. 다음 식들을 모두 합산해보면 위의 식을 얻게 된다.  
>
><div class="math"><script type="math/tex; mode=display">
>\begin{aligned}
>\mathbf{H}[X_n \mid X_1, \cdots, X_{n-1}] &= \mathbf{H}[X_1, \cdots, X_n] - \mathbf{H}[X_1, \cdots, X_{n-1}] \\
>\mathbf{H}[X_{n-1} \mid X_1, \cdots, X_{n-2}] &= \mathbf{H}[X_1, \cdots, X_{n-1}] - \mathbf{H}[X_1, \cdots, X_{n-2}] \\
>&~~\vdots \\
>\mathbf{H}[X_{2} \mid X_1] &= \mathbf{H}[X_1, X_{2}] - \mathbf{H}[X_1] \\
>\mathbf{H}[X_{1}] &= \mathbf{H}[X_1] \\
>\end{aligned}
></script></div>

<br/>




### 정보이득

[정보이득(Information Gain)](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)은 주어진 정보로 인해 확률변수의 불확실성이 얼마나 감소했는지를 나타내는 지표이다. 정보이득 <span><script type="math/tex">\mathbf{IG}</script></span> 는 다음과 같이 정의된다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{IG}[Y, X] \equiv \mathbf{H}[Y] - \mathbf{H}[Y | X]
</script></div>



<br/>

## KL 다이버전스

[**KL 다이버전스**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (**KLD**: Kullback–Leibler divergence, 쿨백-라이블러 발산)는 하나의 확률변수에 대한 **두 확률분포 간의 차이**를 측정하는 도구 중 하나이며, 상대 엔트로피(Relative entropy) 라고도 한다. 확률분포 <span><script type="math/tex">p</script></span>와 <span><script type="math/tex">q</script></span>에 대하여, 다음과 같이 **엔트로피와 교차 엔트로피 간의 차이**로 정의된다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{D}_{\text{KL}} (p \parallel q) \equiv \mathbf{H}(p,q) - \mathbf{H}(p)
</script></div>

수학적인 의미는, 어떤 확률분포 <span><script type="math/tex">p</script></span>가 있을 때, 그 분포를 근사적으로 추정한 확률분포 <span><script type="math/tex">q</script></span>를 대신 사용했을 경우의 엔트로피 변화를 말한다. 엔트로피와 교차 엔트로피의 정의에 의해, 

<div class="math"><script type="math/tex; mode=display">
\mathbf{D}_{\text{KL}} (p \parallel q) = \mathbf{E}_p \left[ \log p(X) - \log q(X) \right]
</script></div>

임을 알 수 있다. 만약 <span><script type="math/tex">X</script></span>가 이산확률변수이고, 표본공간 <span><script type="math/tex">\mathbb{X}= \{ x_1, \cdots, x_n \}</script></span> 에 대하여 <span><script type="math/tex">p_i = p(x_i)</script></span> 및 <span><script type="math/tex">q_i = q(x_i)</script></span> 이라면, 가장 널리 알려진 다음의 정의를 얻게 된다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{D}_{\text{KL}} (p \parallel q) = \sum_{i=1}^n p_i \log \frac{p_i}{q_i}
</script></div>

한편, KL 다이버전스를 **두 확률분포 간의 거리** 개념으로 이해할 수도 있다. 단 Symmetric 하지는 않다는 점을 명심해야 한다. 즉, 

<div class="math"><script type="math/tex; mode=display">
\mathbf{D}_{\text{KL}} (p \parallel q) \ne \mathbf{D}_{\text{KL}} (q \parallel p)
</script></div>


<br/>


### Information inequality
<span><script type="math/tex">X</script></span>의 두 확률밀도함수 <span><script type="math/tex">p, q</script></span>에 대해서, 다음의 부등식을 **Information inequality** 라고 한다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{D}_{\text{KL}} (p \parallel q) \ge 0
</script></div>

<div class="math"><script type="math/tex; mode=display">
\mathbf{D}_{\text{KL}} (p \parallel q) = 0  \Longleftrightarrow p(x)=q(x)
</script></div>


Information inequality를 이용하면, KL 다이버전스의 정의에 의해 다음의 부등식을 추가적으로 알게 된다. 

<div class="math"><script type="math/tex; mode=display">
\mathbf{H}(p,q) \ge \mathbf{H}(p)
</script></div>


**Proof.**
<span><script type="math/tex">\log</script></span>는 [Strictly concave 함수](https://gem763.github.io/machine%20learning/Affinity%EC%99%80-Convexity.html#strictly-convex-function)이므로, [Jensen 부등식](https://gem763.github.io/machine%20learning/Affinity%EC%99%80-Convexity.html#jesen-%EB%B6%80%EB%93%B1%EC%8B%9D)을 이용하면, 

<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{D}_{\text{KL}} (p \parallel q) 
&= -\mathbf{E}_p \left[ \log \frac{q(X)}{p(X)} \right] \\
&\ge -\log \mathbf{E}_p \left[ \frac{q(X)}{p(X)} \right] \\
&= -\log \left(\sum_x p(x) \frac{q(x)}{p(x)} \right) \\
&=  -\log 1 \\
&= 0
\end{aligned}
</script></div>

여기서는 <span><script type="math/tex">X</script></span>가 이산확률변수라고 가정하였는데, 연속확률변수인 경우에도 같은 논리를 적용할 수 있다. 이를테면 <span><script type="math/tex">\mathbf{E}_p \left[ \frac{q(X)}{p(X)} \right] = \int_x p(x) \frac{q(x)}{p(x)}dx = \int_x q(x) dx = 1</script></span> 이므로, 결국 위와 동일한 부등식을 얻을 수 있게 된다. 

한편 등호는 <span><script type="math/tex">\frac{q(X)}{p(X)}</script></span>가 상수일 때 성립한다. <span><script type="math/tex">\frac{q(X)}{p(X)} = c</script></span> 에서

<div class="math"><script type="math/tex; mode=display">
1 = \sum_x q(x) = c \sum_x p(x) = c
</script></div>

따라서 모든 <span><script type="math/tex">x</script></span>값에 대하여 <span><script type="math/tex">p(x) = q(x)</script></span> 가 된다. 


<br/>

다음 차트는 다양한 위치의 두 정규분포에 대하여 KL 다이버전스를 측정해 본 결과이다. 


<center><img src="https://cdn-ak.f.st-hatena.com/images/fotolife/y/yusuke_ujitoko/20170507/20170507192925.png" alt="KLD"/></center>
<center><small>(출처: http://yusuke-ujitoko.hatenablog.com)</small></center>

<br/>




### 예시
아래 표에서 <span><script type="math/tex">P</script></span>는 [이항분포(Binomial distribution)](https://gem763.github.io/probability%20theory/%EB%B2%A0%EB%A5%B4%EB%88%84%EC%9D%B4-%EB%B6%84%ED%8F%AC%EC%99%80-%EC%9D%B4%ED%95%AD%EB%B6%84%ED%8F%AC.html#%EC%9D%B4%ED%95%AD%EB%B6%84%ED%8F%AC), <span><script type="math/tex">Q</script></span>는 [균등분포(Uniform distribution)]((https://en.wikipedia.org/wiki/Discrete_uniform_distribution))을 나타낸다. 

|  | 1 | 2 | 3 |
|:--:|:--:|:--:|:--:|
| <span><script type="math/tex">P</script></span> | 0.36 | 0.48 | 0.16 | 
| <span><script type="math/tex">Q</script></span> | 0.333 | 0.333 | 0.333  |

<center><small>(출처: 위키피디아)</small></center>

<center><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Kullback%E2%80%93Leibler_distributions_example_1.svg/570px-Kullback%E2%80%93Leibler_distributions_example_1.svg.png" alt="KLD"/></center>


<div class="math"><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{D}_{\text{KL}}(P \parallel Q) 
&= \sum_i P(i) \ln \tfrac{P(i)}{Q(i)} \\
&= 0.36 \ln \tfrac{0.36}{0.333} + 0.48 \ln \tfrac{0.48}{0.333} + 0.16 \ln \tfrac{0.16}{0.333} \\
&= 0.0853 \\[6pt]
\mathbf{D}_{\text{KL}}(Q \parallel P) 
&= \sum_i Q(i) \ln \tfrac{Q(i)}{P(i)} \\
&= 0.333 \ln \tfrac{0.333}{0.36} + 0.333 \ln \tfrac{0.333}{0.48} + 0.333 \ln \tfrac{0.333}{0.16} \\
&= 0.0975
\end{aligned}
</script></div>



